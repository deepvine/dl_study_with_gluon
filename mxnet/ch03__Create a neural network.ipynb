{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial refers to https://gluon-crash-course.mxnet.io/nn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"nd\" means the NDArray package and<br>\n",
    "\"nn\" means the neural network package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a first layer of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dense layer with 2 output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(None -> 2, linear)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Dense(2)\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize its weights with the default initialization method, which draws random values uniformly from [−0.7,0.7]<br>\n",
    "TODO: So what size does it have internally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(None -> 2, linear)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.initialize()\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " create a (3,4) shape random input x which draws random values uniformly from [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.09762704  0.18568921  0.43037868  0.6885315 ]\n",
       " [ 0.20552671  0.71589124  0.08976638  0.6945034 ]\n",
       " [-0.15269041  0.24712741  0.29178822 -0.23123658]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.random.uniform(-1,1,(3,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.02524133 -0.00874885]\n",
       " [-0.06026538 -0.01308061]\n",
       " [ 0.02468396 -0.02181557]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the layer’s input limit of 2 produced a (3,2) shape output from our (3,4) input.<br>\n",
    "the system will automatically infer it during the first time we feed in data, create and initialize the weights.<br>\n",
    "so, inside the layer's size is (4,2)\n",
    "\n",
    "(3,4) * (4,2) = (3,2)\n",
    "\n",
    "we can access the weight after the first forward pass<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.00873779 -0.02834515  0.05484822 -0.06206018]\n",
       " [ 0.06491279 -0.03182812 -0.01631819 -0.00312688]]\n",
       "<NDArray 2x4 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chain layers into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a simple case that a neural network is a chain of layers.<br>\n",
    "During the forward pass, it runs layers sequentially one-by-one.<br>\n",
    "The following code implements a famous network called LeNet through nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    # Similar to Dense, it is not necessary to specify the\n",
    "    # input channels by the argument `in_channels`, which will be\n",
    "    # automatically inferred in the first forward pass. Also,\n",
    "    # we apply a relu activation on the output.\n",
    "    #\n",
    "    # In addition, we can use a tuple to specify a\n",
    "    # non-square kernel size, such as `kernel_size=(2,4)`\n",
    "    nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "    # One can also use a tuple to specify non-symmetric\n",
    "    # pool and stride sizes\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    # flatten the 4-D input into 2-D with shape\n",
    "    # `(x.shape[0], x.size/x.shape[0])` so that it can be used\n",
    "    # by the following dense layers\n",
    "    nn.Flatten(),\n",
    "    nn.Dense(120, activation=\"relu\"),\n",
    "    nn.Dense(84, activation=\"relu\"),\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (2): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (4): Flatten\n",
       "  (5): Dense(None -> 120, Activation(relu))\n",
       "  (6): Dense(None -> 84, Activation(relu))\n",
       "  (7): Dense(None -> 10, linear)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of nn.Sequential is similar to nn.Dense.<br>\n",
    "In fact, both of them are subclasses of nn.Block. <br>\n",
    "The following codes show how to initialize the weights and run the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "x = nd.random.uniform(shape=(4,1,28,28)) # Input shape is (batch_size, color_channels, height, width)\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following accesses the 1st layer’s weight and 6th layer’s bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 1, 5, 5), (120,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net[0].weight.data().shape, net[5].bias.data().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a neural network flexibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another way to construct a network with a flexible forward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do it, first create a subclass of nn.Block and implement two methods:\n",
    "\n",
    "* \\_\\_init\\_\\_ create the layers\n",
    "* forward define the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MixMLP, self).__init__(**kwargs)\n",
    "        self.blk = nn.Sequential()\n",
    "        self.blk.add(nn.Dense(3, activation='relu'), #2,3\n",
    "                    nn.Dense(4, activation='relu')) #3,4\n",
    "        self.dense = nn.Dense(5) #4,5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = nd.relu(self.blk(x))\n",
    "        print(y)\n",
    "        return self.dense(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixMLP(\n",
       "  (blk): Sequential(\n",
       "    (0): Dense(None -> 3, Activation(relu))\n",
       "    (1): Dense(None -> 4, Activation(relu))\n",
       "  )\n",
       "  (dense): Dense(None -> 5, linear)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MixMLP()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[6.2898183e-01 5.5350363e-05]\n",
       " [8.7265068e-01 3.1186023e-01]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "x = nd.random.uniform(shape=(2,2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "access a particular layer’s weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.06339083 -0.00614183  0.02624836]\n",
       " [-0.00232279 -0.03982893  0.04042352]\n",
       " [ 0.06263188 -0.03787814  0.03231981]\n",
       " [ 0.05324166 -0.03444817 -0.02608307]]\n",
       "<NDArray 4x3 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.blk[1].weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
